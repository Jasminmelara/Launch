{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating Text with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "may need to update nltk after initially installing it: **pip3 install -U nltk**\n",
    "\n",
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms to know:\n",
    "\n",
    "Tokenizing - word tokenizers, sentence tokenizers\n",
    "\n",
    "Stop Words - meaningless words (or filler words) such as \"like\", \"as\", \"the\", \"it\", etc…\n",
    "\n",
    "Corpora (Corpus) - body of text. ex: medical journals, presidential speeches, etc…\n",
    "\n",
    "Lexicon - words and their meanings. ex: \"well\" can beither an adjective or a noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lk6me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized by Sentence:\n",
      "\n",
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great and python is awesome.', 'The sky is pinkish-blue.', 'You should not eat cardboard.']\n",
      "\n",
      "Tokenized by word:\n",
      "\n",
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', 'not', 'eat', 'cardboard', '.']\n",
      "\n",
      "Tokenized by word without punctuation:\n",
      "\n",
      "['Hello', 'Mr', 'Smith', 'how', 'are', 'you', 'doing', 'today', 'The', 'weather', 'is', 'great', 'and', 'python', 'is', 'awesome', 'The', 'sky', 'is', 'pinkish', 'blue', 'You', 'should', 'not', 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_text = 'Hello Mr. Smith, how are you doing today? The weather is great and python is awesome. The sky is pinkish-blue. You should not eat cardboard.'\n",
    "\n",
    "# tokenize by sentence\n",
    "print('Tokenized by Sentence:\\n')\n",
    "print(sent_tokenize(example_text))\n",
    "\n",
    "\n",
    "# tokenize by word\n",
    "print('\\nTokenized by word:\\n')\n",
    "print(word_tokenize(example_text))\n",
    "# notice that this counts punctuation as its own word. You can change this parameter if you so choose.\n",
    "\n",
    "# if you want to tokenize the words but exclude punctuation, use this RegexpTokenizer():\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "print('\\nTokenized by word without punctuation:\\n')\n",
    "print(tokenizer.tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words:\n",
      "\n",
      "{'y', 'by', 'won', 'such', 'all', 'is', \"won't\", \"mightn't\", 'below', 'couldn', \"isn't\", 'before', 'shan', \"you'd\", 'has', 'being', 'does', 'did', 'only', 'both', 'because', \"aren't\", 'me', 'be', 'ma', 'shouldn', \"wasn't\", 'what', \"hadn't\", 'here', 'some', 'who', 'where', 'your', 'ourselves', 'again', 'a', 's', 'my', \"mustn't\", 'yourselves', 'herself', \"didn't\", \"shan't\", \"she's\", 'this', 'during', 'that', \"you'll\", 'most', 'no', 'don', 'nor', 'd', \"you're\", 'them', 'into', 'now', 'of', 'her', 'but', 'didn', 'his', 'to', 'was', 'were', 'how', 'can', \"haven't\", 'isn', 'he', 'about', 'more', 'under', 'myself', \"it's\", 'over', 're', \"hasn't\", 'you', 'down', 't', 'between', \"needn't\", 'with', 'for', 'further', 'yours', 'him', 'theirs', 'himself', 'as', 'off', 'their', 'had', 'own', 'above', 'mightn', 'wasn', 'up', 'it', 'are', 'against', 'yourself', 'have', 'there', 'after', 'm', 'same', 've', 'when', 'too', 'weren', \"wouldn't\", 'mustn', 'i', \"shouldn't\", \"couldn't\", 'they', 'doing', 'then', 'at', 'than', 'hasn', 'we', 'ain', 'which', 'on', 'an', 'or', 'will', 'doesn', 'wouldn', 'aren', 'she', 'so', 'while', 'do', 'not', 'these', 'very', 'any', 'having', 'out', 'few', 'o', 'those', 'and', 'in', 'been', \"don't\", 'the', 'other', 'through', 'needn', 'each', \"should've\", 'itself', 'ours', 'from', 'haven', 'should', 'whom', 'themselves', 'if', 'until', 'why', 'its', 'hadn', \"doesn't\", \"weren't\", \"that'll\", 'once', 'am', \"you've\", 'just', 'll', 'our', 'hers'}\n",
      "\n",
      "Examples of removing stop words:\n",
      "\n",
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n",
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "example_sentence = 'This is an example showing off stop word filtration.'\n",
    "\n",
    "# NLTK has multiple sets of stop words, but we are going to use the set made specifically for the English language.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print('Stop words:\\n')\n",
    "print(stop_words)\n",
    "\n",
    "print('\\nExamples of removing stop words:\\n')\n",
    "      \n",
    "words = word_tokenize(example_sentence)\n",
    "\n",
    "# non-fancy way to filter the stop words out of your text:\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)\n",
    "# notice that only the \"important\" words are left\n",
    "\n",
    "# fancy way to filter the stop words out of your text:\n",
    "filtered_sentence = [word for word in words if not word in stop_words] # basically the for-loop in a single line\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stemming\n",
    "\n",
    "Taking words and \"stemming\" the ends of them. Such as \"-ed\", or \"-ing\" at the end of a word.\n",
    "\n",
    "So, the words \"worked\" and \"working\" would both be converted to \"work\" because they basically have the same root meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example words:\n",
      "\n",
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "\n",
      "Stemmed sentence:\n",
      "\n",
      "It\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = ['python','pythoner','pythoning','pythoned','pythonly']\n",
    "\n",
    "print('Example words:\\n')\n",
    "for word in example_words:\n",
    "    print(ps.stem(word))\n",
    "\n",
    "print('\\nStemmed sentence:\\n')\n",
    "# Lets see what a stemmed sentence looks like\n",
    "sent_to_stem = 'It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.'\n",
    "\n",
    "words = word_tokenize(sent_to_stem)\n",
    "# we tokenize because PorterStemmer can only stem one word at a time, not an entire sentence.\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming can sometimes act strangely with \"-ly\" endings and, as you can see above, the word \"once\", but overall serves its purpose to take different forms of words and convert them to their root meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\lk6me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lk6me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PRESIDENT', 'NNP'),\n",
       " ('GEORGE', 'NNP'),\n",
       " ('W.', 'NNP'),\n",
       " ('BUSH', 'NNP'),\n",
       " (\"'S\", 'POS'),\n",
       " ('ADDRESS', 'NNP'),\n",
       " ('BEFORE', 'IN'),\n",
       " ('A', 'NNP'),\n",
       " ('JOINT', 'NNP'),\n",
       " ('SESSION', 'NNP'),\n",
       " ('OF', 'IN'),\n",
       " ('THE', 'NNP'),\n",
       " ('CONGRESS', 'NNP'),\n",
       " ('ON', 'NNP'),\n",
       " ('THE', 'NNP'),\n",
       " ('STATE', 'NNP'),\n",
       " ('OF', 'IN'),\n",
       " ('THE', 'NNP'),\n",
       " ('UNION', 'NNP'),\n",
       " ('January', 'NNP')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "nltk.download('state_union')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import state_union # imports a pre-loaded corpus with a bunch of state of the union addresses\n",
    "\n",
    "# get our state of the union address to analyze\n",
    "sample_text = state_union.raw(\"2006-GWBUSH.txt\")\n",
    "\n",
    "# tokenize each word of the text\n",
    "words = nltk.word_tokenize(sample_text)\n",
    "\n",
    "# use nltk.pos_tag() on the words to get their parts of speech in one long list\n",
    "tagged = nltk.pos_tag(words)\n",
    "\n",
    "tagged[0:20] # only printed the first 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part of speech tags:\n",
    "\n",
    "CC - coordinating conjunction<br>\n",
    "CD - cardinal digit<br>\n",
    "DT - determiner<br>\n",
    "EX - existential there (like: \"there is\" ... think of it like \"there exists\")<br>\n",
    "FW - foreign word<br>\n",
    "IN - preposition/subordinating conjunction<br>\n",
    "JJ - adjective\t'big'<br>\n",
    "JJR - adjective, comparative\t'bigger'<br>\n",
    "JJS - adjective, superlative\t'biggest'<br>\n",
    "LS - list marker\t1)<br>\n",
    "MD - modal\tcould, will<br>\n",
    "NN - noun, singular 'desk'<br>\n",
    "NNS - noun plural\t'desks'<br>\n",
    "NNP - proper noun, singular\t'Harrison'<br>\n",
    "NNPS - proper noun, plural\t'Americans'<br>\n",
    "PDT - predeterminer\t'all the kids'<br>\n",
    "POS - possessive ending\tparent's<br>\n",
    "PRP - personal pronoun\tI, he, she<br>\n",
    "PRP\\$ - possessive pronoun\tmy, his, hers<br>\n",
    "RB - adverb\tvery, silently<br>\n",
    "RBR - adverb, comparative\tbetter<br>\n",
    "RBS - adverb, superlative\tbest<br>\n",
    "RP - particle\tgive up<br>\n",
    "TO - to\tgo 'to' the store.<br>\n",
    "UH - interjection\terrrrrrrrm<br>\n",
    "VB - verb, base form\ttake<br>\n",
    "VBD - verb, past tense\ttook<br>\n",
    "VBG - verb, gerund/present participle\ttaking<br>\n",
    "VBN - verb, past participle\ttaken<br>\n",
    "VBP - verb, sing. present, non-3d\ttake<br>\n",
    "VBZ - verb, 3rd person sing. present\ttakes<br>\n",
    "WDT - wh-determiner\twhich<br>\n",
    "WP - wh-pronoun\twho, what<br>\n",
    "WP$ - possessive wh-pronoun\twhose<br>\n",
    "WRB - wh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\lk6me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "955610\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews # imports a corpus with a bunch of movie reviews\n",
    "import re\n",
    "\n",
    "\n",
    "# so that we are not finding the frequency of more or less useless things, lets remove stop words and punctuation\n",
    "# all of the words in the corpus:\n",
    "words = movie_reviews.words()\n",
    "\n",
    "# removing stop words:\n",
    "stop_words = set(stopwords.words('english')) # we make this a set because sets are faster than lists\n",
    "filtered_words = [word for word in words if not word in stop_words]\n",
    "print(len(filtered_words))\n",
    "\n",
    "\n",
    "# define \"all_words\" as a list that will be all of the words in the corpus (converted to lowercase)\n",
    "all_words = []\n",
    "for word in filtered_words: # movie_reviews.words() is a list of all of the documents tokenized by word\n",
    "    # lets stem and convert the words to lowercase so that we avoid duplicated words and get more meaningful results\n",
    "    if re.match('[a-zA-Z]+', word): # filtering out punctuation\n",
    "        try:\n",
    "            # stem the word\n",
    "            word = ps.stem(word)\n",
    "        except IndexError: \n",
    "            pass\n",
    "        finally:\n",
    "            # convert word to lowercase and append it to all_words\n",
    "            all_words.append(word.lower())\n",
    "\n",
    "print(all_words)\n",
    "\n",
    "# define \"word_frequencies\" as the frequencies of all of the words in \"all_words\"\n",
    "word_frequencies = nltk.FreqDist(all_words)\n",
    "# print the 20 most common words\n",
    "print(word_frequencies.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
