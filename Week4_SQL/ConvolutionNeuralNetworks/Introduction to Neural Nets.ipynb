{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network introduction video: https://www.youtube.com/watch?v=p69khggr1Jo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you (kind of) know what's going on, let's try visualizing things on your own.\n",
    "Tensorflow, created by Google, is one of the most widely used libraries for deep learning. They also have a \"playgound\" where you can mess around with naive datasets and features. See if you can choose the right features and parameters, while designing the architecture of your own Neural Network, to try to minimize the error:\n",
    "\n",
    "https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a python library called Keras. Why not Tensorflow you ask? Tensorflow is designed for full scale deep learning, where you have total control over your model. This sounds great, but also is much more complicated and requires alot of code. Enter Keras. Keras acts as a \"wrapper\" for Tensorflow, where it uses Tensorflow in the behind the scenes, but it does all of the hard work for you. Thus we must have Keras AND Tensorflow installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /anaconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six>=1.9.0 in /anaconda/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /anaconda/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /anaconda/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in /anaconda/lib/python3.6/site-packages (from keras)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /anaconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: protobuf>=3.3.0 in /anaconda/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /anaconda/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: tensorflow-tensorboard<0.2.0,>=0.1.0 in /anaconda/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: setuptools in /anaconda/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg (from protobuf>=3.3.0->tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /anaconda/lib/python3.6/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Collecting html5lib==0.9999999 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 718kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bleach==1.5.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda/lib/python3.6/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Building wheels for collected packages: html5lib\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/Charlie/Library/Caches/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
      "Successfully built html5lib\n",
      "Installing collected packages: html5lib, bleach\n",
      "  Found existing installation: html5lib 1.0.1\n",
      "    Uninstalling html5lib-1.0.1:\n",
      "      Successfully uninstalled html5lib-1.0.1\n",
      "  Found existing installation: bleach 2.1.3\n",
      "    Uninstalling bleach-2.1.3:\n",
      "      Successfully uninstalled bleach-2.1.3\n",
      "Successfully installed bleach-1.5.0 html5lib-0.9999999\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# reference: https://keras.io/#installation\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data comes from a classic ML problem concerning Pima Native Americans and predicting cases of Diabetes using 8 different health-related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "RANDOM_SEED = 7\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# load pima indians dataset\n",
    "# make sure csv is in same directory as this notebook\n",
    "dataset = np.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "# Dense layers are \"fully connected layers\"\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sooo wtf is an activation function? 4 reasons for them (there are more, probably):\n",
    "\n",
    "1. Introducing non-linearity into our model, otherwise it's just a fat linear model\n",
    "2. Determining/restricting the output of our model\n",
    "3. Helps with exploding/vanishing gradients\n",
    "4. Decides when a neuron should \"fire\"\n",
    "\n",
    "Read more here (awesome blog in general): https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nn_img.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "691/691 [==============================] - 0s 581us/step - loss: 0.4962 - acc: 0.7670\n",
      "Epoch 2/100\n",
      "691/691 [==============================] - 0s 166us/step - loss: 0.4993 - acc: 0.7525\n",
      "Epoch 3/100\n",
      "691/691 [==============================] - 0s 146us/step - loss: 0.4751 - acc: 0.7829\n",
      "Epoch 4/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4878 - acc: 0.7670\n",
      "Epoch 5/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4810 - acc: 0.7858\n",
      "Epoch 6/100\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.5098 - acc: 0.7467\n",
      "Epoch 7/100\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4957 - acc: 0.7728\n",
      "Epoch 8/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5593 - acc: 0.7279\n",
      "Epoch 9/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.4874 - acc: 0.7800\n",
      "Epoch 10/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4897 - acc: 0.7612\n",
      "Epoch 11/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 0.4693 - acc: 0.7728\n",
      "Epoch 12/100\n",
      "691/691 [==============================] - 0s 180us/step - loss: 0.4831 - acc: 0.7844\n",
      "Epoch 13/100\n",
      "691/691 [==============================] - 0s 137us/step - loss: 0.4709 - acc: 0.7713\n",
      "Epoch 14/100\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4921 - acc: 0.7569\n",
      "Epoch 15/100\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4854 - acc: 0.7728\n",
      "Epoch 16/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4790 - acc: 0.7757\n",
      "Epoch 17/100\n",
      "691/691 [==============================] - 0s 137us/step - loss: 0.4934 - acc: 0.7598\n",
      "Epoch 18/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4832 - acc: 0.7713\n",
      "Epoch 19/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 0.4786 - acc: 0.7713\n",
      "Epoch 20/100\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4894 - acc: 0.7685\n",
      "Epoch 21/100\n",
      "691/691 [==============================] - 0s 158us/step - loss: 0.4784 - acc: 0.7699\n",
      "Epoch 22/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 0.4741 - acc: 0.7627\n",
      "Epoch 23/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.5120 - acc: 0.7525\n",
      "Epoch 24/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4824 - acc: 0.7554\n",
      "Epoch 25/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4792 - acc: 0.7728\n",
      "Epoch 26/100\n",
      "691/691 [==============================] - 0s 144us/step - loss: 0.4879 - acc: 0.7496\n",
      "Epoch 27/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4761 - acc: 0.7713\n",
      "Epoch 28/100\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4856 - acc: 0.7670\n",
      "Epoch 29/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 0.5108 - acc: 0.7453\n",
      "Epoch 30/100\n",
      "691/691 [==============================] - 0s 144us/step - loss: 0.5368 - acc: 0.7308\n",
      "Epoch 31/100\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.4825 - acc: 0.7641\n",
      "Epoch 32/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4864 - acc: 0.7699\n",
      "Epoch 33/100\n",
      "691/691 [==============================] - 0s 145us/step - loss: 0.4806 - acc: 0.7627\n",
      "Epoch 34/100\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.4891 - acc: 0.756 - 0s 142us/step - loss: 0.4798 - acc: 0.7656\n",
      "Epoch 35/100\n",
      "691/691 [==============================] - 0s 142us/step - loss: 0.4911 - acc: 0.7482\n",
      "Epoch 36/100\n",
      "691/691 [==============================] - 0s 140us/step - loss: 0.4887 - acc: 0.7540\n",
      "Epoch 37/100\n",
      "691/691 [==============================] - 0s 149us/step - loss: 0.5222 - acc: 0.7612\n",
      "Epoch 38/100\n",
      "691/691 [==============================] - 0s 139us/step - loss: 0.5244 - acc: 0.7294\n",
      "Epoch 39/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 0.4758 - acc: 0.7699\n",
      "Epoch 40/100\n",
      "691/691 [==============================] - 0s 141us/step - loss: 0.4715 - acc: 0.7800\n",
      "Epoch 41/100\n",
      "691/691 [==============================] - 0s 143us/step - loss: 0.4726 - acc: 0.7685\n",
      "Epoch 42/100\n",
      "691/691 [==============================] - 0s 138us/step - loss: 0.4900 - acc: 0.7627\n",
      "Epoch 43/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4749 - acc: 0.7757\n",
      "Epoch 44/100\n",
      "691/691 [==============================] - 0s 108us/step - loss: 0.4747 - acc: 0.7713\n",
      "Epoch 45/100\n",
      "691/691 [==============================] - 0s 122us/step - loss: 0.5023 - acc: 0.7670\n",
      "Epoch 46/100\n",
      "691/691 [==============================] - 0s 125us/step - loss: 0.4719 - acc: 0.7728\n",
      "Epoch 47/100\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.4679 - acc: 0.7931\n",
      "Epoch 48/100\n",
      "691/691 [==============================] - 0s 131us/step - loss: 0.5584 - acc: 0.7323\n",
      "Epoch 49/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.5044 - acc: 0.7540\n",
      "Epoch 50/100\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4708 - acc: 0.7815\n",
      "Epoch 51/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4677 - acc: 0.7844\n",
      "Epoch 52/100\n",
      "691/691 [==============================] - 0s 144us/step - loss: 0.4682 - acc: 0.7771\n",
      "Epoch 53/100\n",
      "691/691 [==============================] - 0s 144us/step - loss: 0.4688 - acc: 0.7786\n",
      "Epoch 54/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4642 - acc: 0.7873\n",
      "Epoch 55/100\n",
      "691/691 [==============================] - 0s 107us/step - loss: 0.4948 - acc: 0.7583\n",
      "Epoch 56/100\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.5086 - acc: 0.7381\n",
      "Epoch 57/100\n",
      "691/691 [==============================] - 0s 119us/step - loss: 0.4969 - acc: 0.7540\n",
      "Epoch 58/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4796 - acc: 0.7699\n",
      "Epoch 59/100\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4830 - acc: 0.7742\n",
      "Epoch 60/100\n",
      "691/691 [==============================] - 0s 127us/step - loss: 0.4759 - acc: 0.7685\n",
      "Epoch 61/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.5028 - acc: 0.7569\n",
      "Epoch 62/100\n",
      "691/691 [==============================] - 0s 147us/step - loss: 0.4680 - acc: 0.7815\n",
      "Epoch 63/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4645 - acc: 0.7786\n",
      "Epoch 64/100\n",
      "691/691 [==============================] - 0s 114us/step - loss: 0.4739 - acc: 0.7829\n",
      "Epoch 65/100\n",
      "691/691 [==============================] - 0s 121us/step - loss: 0.4764 - acc: 0.7800\n",
      "Epoch 66/100\n",
      "691/691 [==============================] - 0s 121us/step - loss: 0.4633 - acc: 0.7815\n",
      "Epoch 67/100\n",
      "691/691 [==============================] - 0s 123us/step - loss: 0.4768 - acc: 0.7699\n",
      "Epoch 68/100\n",
      "691/691 [==============================] - 0s 122us/step - loss: 0.4721 - acc: 0.7887\n",
      "Epoch 69/100\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4666 - acc: 0.7858\n",
      "Epoch 70/100\n",
      "691/691 [==============================] - 0s 102us/step - loss: 0.4634 - acc: 0.7887\n",
      "Epoch 71/100\n",
      "691/691 [==============================] - 0s 120us/step - loss: 0.5001 - acc: 0.7525\n",
      "Epoch 72/100\n",
      "691/691 [==============================] - 0s 115us/step - loss: 0.4886 - acc: 0.7685\n",
      "Epoch 73/100\n",
      "691/691 [==============================] - 0s 92us/step - loss: 0.4662 - acc: 0.7887\n",
      "Epoch 74/100\n",
      "691/691 [==============================] - 0s 88us/step - loss: 0.4604 - acc: 0.7815\n",
      "Epoch 75/100\n",
      "691/691 [==============================] - 0s 122us/step - loss: 0.4693 - acc: 0.7815\n",
      "Epoch 76/100\n",
      "691/691 [==============================] - 0s 120us/step - loss: 0.4768 - acc: 0.7757\n",
      "Epoch 77/100\n",
      "691/691 [==============================] - 0s 110us/step - loss: 0.4723 - acc: 0.7742\n",
      "Epoch 78/100\n",
      "691/691 [==============================] - 0s 123us/step - loss: 0.4638 - acc: 0.7873\n",
      "Epoch 79/100\n",
      "691/691 [==============================] - 0s 104us/step - loss: 0.4848 - acc: 0.7699\n",
      "Epoch 80/100\n",
      "691/691 [==============================] - 0s 103us/step - loss: 0.4721 - acc: 0.7815\n",
      "Epoch 81/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4803 - acc: 0.7757\n",
      "Epoch 82/100\n",
      "691/691 [==============================] - 0s 121us/step - loss: 0.4722 - acc: 0.7713\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 122us/step - loss: 0.4663 - acc: 0.7887\n",
      "Epoch 84/100\n",
      "691/691 [==============================] - 0s 120us/step - loss: 0.4724 - acc: 0.7713\n",
      "Epoch 85/100\n",
      "691/691 [==============================] - 0s 137us/step - loss: 0.5906 - acc: 0.7178\n",
      "Epoch 86/100\n",
      "691/691 [==============================] - 0s 124us/step - loss: 0.4663 - acc: 0.7873\n",
      "Epoch 87/100\n",
      "691/691 [==============================] - 0s 115us/step - loss: 0.4755 - acc: 0.7873\n",
      "Epoch 88/100\n",
      "691/691 [==============================] - 0s 122us/step - loss: 0.4690 - acc: 0.7829\n",
      "Epoch 89/100\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4636 - acc: 0.7757\n",
      "Epoch 90/100\n",
      "691/691 [==============================] - 0s 118us/step - loss: 0.4601 - acc: 0.7931\n",
      "Epoch 91/100\n",
      "691/691 [==============================] - 0s 133us/step - loss: 0.4592 - acc: 0.7815\n",
      "Epoch 92/100\n",
      "691/691 [==============================] - 0s 136us/step - loss: 0.4697 - acc: 0.7916\n",
      "Epoch 93/100\n",
      "691/691 [==============================] - 0s 135us/step - loss: 0.4635 - acc: 0.7887\n",
      "Epoch 94/100\n",
      "691/691 [==============================] - 0s 132us/step - loss: 0.4584 - acc: 0.7786\n",
      "Epoch 95/100\n",
      "691/691 [==============================] - 0s 129us/step - loss: 0.4619 - acc: 0.7945\n",
      "Epoch 96/100\n",
      "691/691 [==============================] - 0s 119us/step - loss: 0.4794 - acc: 0.7800\n",
      "Epoch 97/100\n",
      "691/691 [==============================] - 0s 130us/step - loss: 0.4749 - acc: 0.7916\n",
      "Epoch 98/100\n",
      "691/691 [==============================] - 0s 120us/step - loss: 0.4982 - acc: 0.7511\n",
      "Epoch 99/100\n",
      "691/691 [==============================] - 0s 118us/step - loss: 0.4983 - acc: 0.7511\n",
      "Epoch 100/100\n",
      "691/691 [==============================] - 0s 128us/step - loss: 0.4722 - acc: 0.7800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122d842b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "# Here we are finalizing the structure of our model and outlining:\n",
    "# 1. Loss function: how we are calucalting the performance of our model\n",
    "# 2. Optimizer: the function we will use to update the weights and biases while trying to minimize the loss function\n",
    "# 3. Metrics: how we are scoring our model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "# When we are training, we are attempting to learn the weight and bias parameters\n",
    "# 3 choices for training: stochasitc, batch, or mini-batch (most common)\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 75us/step\n",
      "\n",
      "acc: 75.32%\n"
     ]
    }
   ],
   "source": [
    "# switch to train and test\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually there is a lot more work to do on the data cleaning side of things, but this notebook shows how easy it is to prototype a neural net using Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### So why use Neural Networks???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Perform better with larger amounts of data, and we live in a data rich world\n",
    "2. Rise in computational power\n",
    "3. Algorithms and hype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But also a double-edged sword..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Black Box\n",
    "2. Not enough data sometimes\n",
    "3. Computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
